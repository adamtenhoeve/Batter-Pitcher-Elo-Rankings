# CSCI4831FinalProject
Adam Ten Hoeve's Final Project for CU Boulder's Spring 2019 CSCI: 4831 Sabermetrics course.

For my final project, I created a batter-pitcher comparison system, similar to an Elo score, which ranks players based on the outcomes of each at-bat matchup. With this elo ranking, we are able to calculate a probability that a player would win in any given pitcher-batter matchup. To compare the result of this probability, I recorded the actual historical results so we can compare the calculated probability to the actual number of matches that the pitcher won. In the notebook are also calculations for fWAR so we can compare different forms of overall ranking systems. For this statistic, I used the statcast data from 2016 through 2018 because it gave a large enough sample of matchups for each batter and pitcher.

This elo statistic is similar to other ranking statistics in that it tries to score players based on their previous results and give them a numerical score for how well they’re expected to do in the future. The statistic is similar to WAR in that it provides an estimate of how different a player is performing when compared to the rest of the competition. It also takes a similar approach to many predictive scoring algorithms, such as the ESPN rankings from the fantasy draft. However, unlike many of the other ranking systems, the elo ranking doesn’t just calculate a score from the player’s overall output but weights those values based on the perceived ability of their opponent, which can give us a better overall view of a player and the playing field as a whole.
  
After processing all 3 years of the data, we got finalized elo rankings. According to the elo rankings, the top 5 batters (in no particular order) where Jose Altuve, Kris Bryant, Paul Goldschmidt, Mike Trout, and Joey Votto. If we extend that to the top 10, we also include Mookie Betts, DJ Lemahieu, J.D. Martinez, Justin Turner and Christian Yelich. Comparing those player’s WAR statistic on fangraphs shows us that they also agree that (most) of these players are solid in their performance. Looking at the pitchers, our top 5 (again in no particular order) are Jacob deGrom, Josh Hader, Kenley Jansen, Max Scherzer and Justin Verlander. The top 10 extends the list with Walker Buehler, Zack Greinke, Corey Kluber, Roberto Osuna and Chris Sale. Again, comparing to fangraphs fWAR, some similar picks for the top pitchers, and also some odd selections. Note that these elo scores were calculated with the weights to get the outcome proportions to mirror 50% pitchers, 50% hitters. I will talk about this more in the next paragraph. 

![PitcherEloTable](https://user-images.githubusercontent.com/32109781/57010898-52e0d200-6bbc-11e9-99a2-a8691152c6fa.PNG)

![BatterEloTable](https://user-images.githubusercontent.com/32109781/57010910-5e33fd80-6bbc-11e9-8bbc-279140f14c8a.PNG)
  
I must mention that this project did have some issues. The primary one being the elo ranking system itself. In chess, the system assumes that if two people of equal skill were put against each other, then they would have an equal chance of winning the game. That is not true for baseball’s batter-pitcher matchups. From my exploration of the data, I found that pitchers “win” the matchup about 65-70 percent of the time if the elo ratings are equal, assuming we take that actions of the fielders, such as field outs and field errors, into account. If we do not take those values into account, then pitchers only “win” about 30-35 percent of the time. In both of these situations, the resulting elo scores are then heavily skewed towards one side over the other and lead to wonky predictions. I tried to correct for this skew in multiple different ways. I took the log of the elos, I scaled the numbers so they would have the same sample mean and variance, and a few other statistical tricks, but nothing had a positive effect on the eventual calculations. 
  
 In the end, I ended up using a simple weighting scheme. The scheme basically works by taking the proportion of batting wins and pitcher wins, from the total statcast data, and transforming it so each is 50%. I also increased the weight on the prediction calculations so a player with a higher elo wouldn’t be as favored to win than in the basic equations. This worked well enough to get the elo rankings closer together, but there was still some obvious skew between pitchers and batters, as you can see in the plots below. This skew isn’t too prevalent when the data is small (meaning a few months of statcast data) but gets more sever the more data is added. I conclude that my methods is not a perfect solution but I could not find a better answer in my exploration. And maybe that elo rankings for individual players aren’t the best for team sports. In hindsight, it makes sense, but it was worth the investigation.


![EloPlot1](https://user-images.githubusercontent.com/32109781/57010621-d699bf00-6bba-11e9-87c7-979f7dcd6495.png)

If I had more time to work on this, I would’ve smoothed out the skewness mentioned above. I would also try to separate the statistic into more categories, such as a batter’s outcome against left and right handed pitchers or differentiate a player if they played on multiple teams. By creating multiple different elo rankings, it might give more nuance into a player’s strengths and weaknesses than a single number.

